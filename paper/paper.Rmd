---
title: "Communicating human related spatial distributions: A comparison of spatial visualisations using visual inference."
affiliation:
  author-columnar: true         ## one column per author
  #institution-columnar: true  ## one column per institution (multiple autors eventually)
  # wide: true                  ## one column wide author/affiliation fields
  institution:
    - name: Queensland University of Technology
      department: Science and Engineering Faculty
      location: Brisbane, Australia
      email: stephanie.kobakian@qut.edu.au
      mark: 1
      author:
        - name: Stephanie Kobakian
    - name: Monash University
      department: Econometrics and Business Statistics Faculty
      location: Melbourne, Australia
      email: dicook@monash.edu
      mark: 1
      author:
        - name: Dianne Cook
keywords: ["statistics", "visual inference", "geospatial", "population"]
abstract: |
  The abstract goes here.
  On multiple lines eventually.
dev: png
bibliography: mybibfile.bib
output: rticles::ieee_article
editor_options: 
  chunk_output_type: console
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  error = FALSE, 
  message = FALSE)
```


Introduction
=============
<!-- no \IEEEPARstart -->
<!-- (should never be an issue) -->

<!-- General: comparison of displays, motivate hexagon tile map, enough info to motivate aims -->
<!-- geographies on choropleth -->
Geospatial statistics are often presented on the geographic map base. 
A choropleth map is the common display to present aggreagated statistics for geographic units, and they are often used to present statistics regarding the population. 
Creating a choropleth map involves drawing the administrative boundaries and filling them with colour to communicate the value of the statistic. 
In Australia, there are sets of administrative boundaries that define subdivisions of the population at various granularities.
The set of Australia statistical areas presents an example of a heterogrenous distribution of area.
The rural communicates on a much larger geographic space than small inner city communities.
This has the negative effect of incorrectly showing the spatial distribution of the statistic, especially when a spatial distribution is related to the size of the areas, or the population density.

An alternative display can also be used to effectively communicate a spatial distribution for a set of heterogeneous areas.
Viewers of spatial distributions may come to incorrect conclusions.



Motivation
=============

<!-- Find a new display for the Cancer Atlas -->

## The Australian Cancer Atlas

The Australian Cancer Atlas is an online interactive webtool created to explore the burden of cancer on Australian communities. There are many cancer types presented, and they can be explored on an individual or aggregate level. 
The Australian communities are examined at the Statistical Areas at Level 2 (SA2) [@abs2016] used by the Australian Bureau of Statistics. Bayesian spatial smoothing has been applied to incorporate the value of the statistics of the neighbouring areas. Spatial smoothing allows for the protection privacy and gives stability to the estimates. The statistics that can be mapped are the diagnoses (Standardised Incidence Rates) and excess deaths for each SA2, communicated as the difference from the Australian average of the statistics. The values of the statistic for each geographic area is communicated through the colour used to fill the areas, the colours are chosen for the diverging colour scheme. 

The Australian Cancer Atlas communicates the trends in the distributions of cancer over geographic space. It uses a choropleth map display and diverging colour scheme to draw attention to relationships between neighbouring areas. 


Background
=============


## Population focussed displays

Spatial visualisations communicate the distribution of statistics over geographic space. The most common display for spatial data is the choropleth map. However, the issue of using a choropleth map base becomes obvious when considering the distributions.
Map creators have the ability to present spatial statistics in alternative displays that can highlight the population.
This work aims to show that a hexagon tile map display is a viable alternative to the geographic map base for presenting population statistics.
The same data were shown on a choropleth map, and on a hexagon tile map.
Comparing the results of participants who see the choropleth to those who see a hexagon tile map will show that population related distributions are spotted more frequently in a hexagon tile map display.

When presenting population statistics on a geographic map base, the size of the regions can allow errornous conclusions to be drawn about the state of the statistic over the entire population.
This occurs as large regions filled with a consistent colour or pettern can draw the attention of map readers, and small regions are not paid equal attention.
A choropleth map is not the only display that can be used for presenting geospatial data. 
Alternative maps include various cartograms, and tesselated tile maps. They allow other variables to be included in the display to highlight the staistical values of various geographic areas.

<!-- Cartogram -->



<!-- Why not a cartogram-->

<!-- Tesselated tile gram & hexagon tile map-->

 
<!-- Specific problem for Australia -->

<!-- Figure Choro and Hex tile for Aus-->


## Methodology 

## Visual Inference
- Communicating data through visualisations
- Effective displays for types of data
- Protocol for testing the effectiveness


Classical statistical inference involves hypothesis testing, the process of rejecting a null hypothesis in favour of an alternative. This approach relies on data, the appropriate distributions and their assumptions. 
Visual inference null hypothesis: independence in the variables (absence of all features), alternative hypothesis: Relationship between the variables (presence of some feature).
<!--
GTPCCD
In visual inference, data plots are considered to be test statistics, and these are compared with plots of data generated from a null hypothesis using a lineup

The null hypothesis underlying a
lineup is that there is no real structure visible in the data plot, that any
patterns seen are consistent with randomness, or from a known model.
If the null hypothesis is true then the plot of the data will not be distinguishable
from the plots of null data.

Different lineups are created, using the same data, same null plots and
positions within the lineup, but different plot designs.

Comparing how
long viewers take and how accurately they report the feature of interest
will assess which design is better for the task.
-->

<!--
A null dataset
is a sample from the null distribution, i.e. an example of an innocent
dataset, and a null plot is a plot of a null dataset, showing what an
innocent might look like.

testing can be adapted to work visually instead of numerically.[@GIIV]
Graphical Inference for Infovis
Chloropleth maps Is there a spatial trend?

Simulation. We might be interested in a more specific set of hypothesis:
does time increase linearly with distance from target?
Does accuracy decrease exponentially as number of distractors
increases? In those cases we have a probabilistic model and we
can generate null data sets by sampling from the distribution implied
by the model. This approach is used in Section 4.2.
-->

The lineup protocol is used for visual inference testing.
1. simulate null plots 
2. Insert data with structure into a random location
3. Ask uninvolved person to select the most different plot
4. If location is chosen correctly, the existence of a feature is significant at $\alpha = 1/N$.



> "In this framework, plots take on the role of test statistics, and human cognition the role of statistical tests." @SIEDAMD

The line up protocol involves placing a "guilty" data visualisation in a lineup of "innocents". Where the guilty data set contains structure, and the innocents are equivalent to a null data set. 
In a grid of visualisations, an observer is asked to pick the display that is most different, if they select the data set containing structure, they have identified the guilty hidden within the group innocents.
The guilty data is identified as different from the innocent data with probability $1/m$, where $m$ is the number of null plots plus 1 to account account for the guilty data set. When the guilty data set is chosen, the null hypothesis that it was innocent is rejected with a $1/m$ chance or type I error of being wrong.

The lineup protocol can be used in a variety of testing scenarios. The choropleth map is best used for testing spatial structure in a data set.

<!--
The line-up protocol works like a police line-up: the suspect (test
statistic plot) is hidden in a set of decoys. If the observer, who has not
seen the suspect, can pick it out as being noticeably different, there is
evidence that it is not innocent.
-->
  


Study Design
============

This study aims to answer several questions around the presentation of spatial distributions:

1. Are spatial disease trends, that impact highly populated small areas, detected with higher accuracy when viewed in a hexagon tile map display?

2. Are people faster in detecting spatial disease trends, that impact highly populated small areas, when
using a hexagon tile map display?

additional considerations when completing this experimental task included exploration of the difficulty experienced by participants

<!--
 Do people find hexagon tile maps more difficult to read than choropleth maps?
 Are the reasons for choosing a plot different depending on the type of display?
-->

The mean of the detection rate for choropleth map, denoted as $\mu_C$, and the hexagon tile maps, $\mu_H$ will be contrasted. This leads to the following one sided hypothesis for this study:

$H_0$ : $\mu_H$ = $\mu_C$
$H_a$ : $\mu_H$ > $\mu_C$

The detection rate $\hat\pi$ is calculated as the amount of people that made the choice of plot that contained the real data, out of the participants who saw data plot in the lineup of the null data plots.

The difference in the detection rates for the two displays will be compared using the following:

$$ \hat\pi_C - \hat\pi_H \pm t_{1-\alpha,n-1}\sqrt{\hat\pi_C(1-\hat\pi_C)/n_C + \hat\pi_H(1-\hat\pi_H)/n_H} $$

## Experimental design


The most common display for spatial cancer data is the choropleth map.
This will be the comparative visualisation for presenting the lineups [@VVSIALM].
Most geographic distributions will have some degree of spatial autcorrelation between neighbours.
This feature will exist in all plots in the lineup displays, the plot that contains the trend feature shown in only one set of data will also be affected by spatial autocorrelation.
A reasonable amount of null plots $N-1$ in the lineup was chosen to ensure data is well hidden. For the detailed choropleth of Australian SA2 areas, we set $N = 12$ to not overwhelm participants.
A line up protocol was implemented to arrange 12 maps in each display. 
Individual displays were created by a combination of map type, and spatial trend model.


The hypotheses for each lineup are
$H_0$ : All plots look the same
$H_a$ : One plot looks different to the other plots 


Recruited participants to be uninvolved judges with no prior knowledge of the data to avoid discrimination or advantages.
The online crowdsource platform Figure-Eight was used to recruit participants. 

The researchers contrasted the different plot designs, as hexagon tilemap and geography in the lineups were created using the same data, and same null positions within the lineup.


Let $n$ be the number of independent observers and $x_i$ the
number of observers who picked plot $i$, $i = \{1,...,m\}$

Then $x_i, x_2, ..., x_m$ follows a multinomial
distribution$Mult_{\pi_1, \pi_2, ...., \pi_m}(x_i, x_2, ..., x_m)$ with 
$\sum_i \pi_i = 1$, where $\pi_i$ is the probability that plot $i$ is picked by an
observer, which we can estimate as $\hat{\pi}_i = x_i/n$.
The researchers compared the length of time taken, and the accuracy of the participants choices.
The power of a lineup can therefore be estimated as the ratio of correct
identifications $x$ out of $n$ viewings.

## The variables being manipulated and measured

The variables that were changed between groups were the type of plot shown and the trend model.

Each participant was randomly allocated to either Group A or Group B when they begun the survey. This resulted in 42 participants allocated to Group A, and 53 participants allocated to Group B.

    
The levels of the factors measured in the experiment were:
- Map type: *Choropleth, Hexagon tile*
- Trend: *Locations in three population centres, Locations in multiple population centres, South-East to North-West*

Factor combinations examined by each participant amount to 6 (2x3) lineup displays.
A participant did see the same data for both map types. Four simulated sets of data were generated for each treatment.
This will generate 24 lineups (12 were geographic maps, and 12 were hexagon tile maps). Participants will evaluate 12 lineups, 6 of each map type. Appendix A shows the experimental design visually.
For each of the six geographic displays and six hexagon displays, two of each trend model were shown to participants.

\begin{table}[]
\caption{The Experimental Design}
\label{tab:my-table}
\begin{tabular}{|l|l|l|}
\hline
Trend & Map type & Replicates \\ \hline
NW-SE & Choropleth & 2 \\ \hline
 & Hexagon tile & 2 \\ \hline
Three cities & Choropleth & 2 \\ \hline
 & Hexagon tile & 2 \\ \hline
All cities & Choropleth & 2 \\ \hline
 & Hexagon tile & 2 \\ \hline
\end{tabular}
\end{table}


The variables measured as a result of the changes were the probability of detection each display and the time taken to submit responses.
To measure the accuracy of the detections, the plot chosen for each lineup evaluated was compared to the position of the real spatial trend plot in the lineup. A correct result occurs when the chosen plot matches the position of the real plot, this was recorded in an additional binary variable; 1 = correct; 0 = incorrect.
High efficiency occurs when a small amount of time is taken to evaluate each lineup. This will be measured as the numeric variable measuring the length of time taken to submit the answers to the evaluation of each line up.

## Simulation process

The underlying spatial correlation model was created to provide spatial autocorrelation between neighbouring areas using the longitude and latitude values for the Statistical Areas.
formula = z ~ 1, locations = ~ longitude + latitude

Simulated spatially dependent data using the model on the centroids of each area, for 12 null plots in 12 lineups.

12 sets of data were created.
In these 12 sets of data, each of the 144 maps were smoothed several times to replicate the spatial autocorrelation seen in cancer data sets presented in the Australian Cancer Atlas.

For each of the 144 individual maps, the values attributed to each geographic area are rescaled to show a similar colour scale from deep blue to dark red within each map.

A random location was selected for each set of lineup data.
In this location, a trend model was overlaid on the null set of spatially correlated data.
Each set of lineup data was used to produce a choropleth maps and hexagon tile maps. These matched pairs were split between Group A and Group B.

## Participants

There were 95 participants involved in the study. 
We recruited participants using the Figure-Eight crowd source platform by advertising this survey to participants that fulfilled the following crtieria:

- level 2 or level 3 on the Figure-Eight Platform.
- at least 18 years old

Participants then selected our task from the list of tasks available to them.

Each participant was trained using three test displays orienting them to the evaluation task.
Participants then proceeded to the survey, this involved evaluating 12 displays.

## Experiment procudure and data collection

The participant answered demographic questions and provided consent before evaluating the lineups.

Demographics were collected regarding the study participants:
- Gender (female / male / other),
- Degree education level achieved (high school / bachelors / masters / doctorate / other),
- Age range (18-24 / 25-34 / 35-44 / 45-54 / 55+ / other)
- Lived at least for one year in Australia (Yes / No )

Participants then moved to the evaulation phase.
The set of images differed for Group A and Group B.
After being allocated to a group, each individual was shown the 12 displays in randomised order.

Three questions were asked regarding each display:
- Plot choice
- Reason
- Difficulty

After completing the 12 evaluations, the participants were asked to submit their responses.

Data was collected through a web application containing the online survey.
Each participant used the internet to access the survey.
The data collection took place using a secure link between the survey web application and the googlesheet used to store results. The application would first connect to the googlesheet using the googlesheets [@sheets] R package, and interacted again at the completion of the survey by adding the participant's responses to the 12 displays as 12 rows of data in the googlesheet.

## The methods of data analysis used
The data analysis methods used in order to analyse and collate the results included downloading the survey submissions and opening them into the analysis software R [@RCore].
   
For each of the 12 lineup displays the researchers calculated:
- accuracy: the proportion of subjects who detected the data plot
- efficiency: average time taken to respond

### Visualisations
Side-by-side dot plots were made of accuracy (efficiency) against map type, facetted by trend model type.

Similar plots were made of the feedback and demographic variables - reason for choice, reported difficulty, gender, age, education, having lived in Australia - against the design variables.

Plots will be made in R (R Core Team 2019), with the ggplot2 package (Wickham 2016).   
   
   
### Modeling


The results will be analysed using a generalised linear model, with a subject random effect to account for differences in individuals. There will be two main effects: map type and trend model, which gives the fixed effects part of the model to be

$$\widehat{y_{ij}} = \mu + \tau_i + \delta_j + (\tau\delta)_{ij}, ~~~ i=1,2; ~j=1,2,3$$

where $y_{ij} = 0, 1$ whether the subject detected the data plot, $\mu$ is the overall mean, $\tau_i, i=1,2$ is the map type effect, $\delta_j$ is the trend model effect. We are allowing for an interaction between map type and trend model. Because the response is binary, a logistic model is used. 

A similar model will be constructed for the efficiency, using a log time, and normal errors. 

The feedback and demographic variables will possibly be incorporated as covariates.

Computation will be done using R [@RCore], with the `lme4` package [@lme4].


## Limitations of the data collection


This required internet connection for participants to access the survey
    


Results
============

```{r data}
# Load Libraries
library(tidyverse)
library(lubridate)
library(broom)
library(readxl)
library(lme4)
library(ggthemes)
library(RColorBrewer)
library(visreg)
library(emmeans)

trend_colours <- c(
  "NW-SE" = "#B2DF8A",
  "three cities" = "#A6CEE3",
  "all cities" = "#1F78B4")
  
type_colours <- c(
  "Choro." = "#fcae91",
  "Hex." = "#a50f15")

detect_colours <- c(
  "No" = "#66C2A5",
  "Yes" = "#FC8D62")

  # Downloaded data
d <- read_xlsx("data/experiment-export.xlsx", sheet=2) %>%
  filter(!is.na(contributor)) %>%
  mutate(contributor = factor(contributor))

# Check data set 
# Need to clean multiple entries, 48, 24
# remove duplicated entries due to submit button
d <- d %>% group_by(group, contributor, image_name) %>%
  slice(1) %>% ungroup() %>% 
  arrange(group, contributor, plot_order)

# Remove contributors who did not provide answers to most questions
keep <- d %>% count(contributor, sort = TRUE) %>% filter(n > 10)
d <- d %>% 
  filter(contributor %in% keep$contributor) %>%
  filter(contributor != "1234567890")

# Remove contributors who did not provide any choices
bad_contribs <- d %>% group_by(contributor) %>% 
  summarise(sum0 = sum(choice)) %>% 
  filter(sum0 == 0) %>% 
  pull(contributor)

d <- d %>% 
  filter(!(contributor %in% bad_contribs))


n_contributors <- d %>% count(contributor, sort=TRUE) %>% 
  summarise(n_contributors = length(contributor))

d <- d %>% mutate(certainty = factor(as.character(certainty),
  levels = c("1", "2", "3", "4","5"), ordered=TRUE))
```

The survey responses from participants were kept only if the participant submitted answers for all 12 displays. This resulted in 92 participants.

```{r reps}
replicate <- tibble(image_name = c("aus_cities_12_geo.png", "aus_cities_12_hex.png", 
                                   "aus_cities_3_geo.png", "aus_cities_3_hex.png",
                                   "aus_cities_4_geo.png", "aus_cities_4_hex.png",
                                   "aus_cities_9_geo.png", "aus_cities_9_hex.png",
                                   "aus_nwse_2_geo.png", "aus_nwse_2_hex.png",
                                   "aus_nwse_3_geo.png", "aus_nwse_3_hex.png",
                                   "aus_nwse_5_geo.png", "aus_nwse_5_hex.png",
                                   "aus_nwse_6_geo.png", "aus_nwse_6_hex.png",
                                   "aus_three_12_geo.png", "aus_three_12_hex.png",
                                   "aus_three_5_geo.png", "aus_three_5_hex.png",
                                   "aus_three_8_geo.png", "aus_three_8_hex.png",
                                   "aus_three_9_geo.png", "aus_three_9_hex.png"),
                    replicate = c(1, 1, 2, 2, 3, 3, 4, 4, 
                                  1, 1, 2, 2, 3, 3, 4, 4,
                                  1, 1, 2, 2, 3, 3, 4, 4))
# Add rep info to data
d <- d %>% left_join(., replicate, by = "image_name")
```

The contributors who gave various plot choices and reasons for the twelve displays were kept.
The contributors who detected no plots correctly were analysed further. Three of these contribuotrs gave no choices for any of the twelve displays. They were also removed for the rest of the analysis.

Of the 92 participants, 67 were male, and 25 female. Only two of the participants had lived in Australia before.
The education level achieved by the participants was 70 Bachelors or Masters degree.

# Accuracy

The detection rate for participants reporting the real data trend model is used to calculate the accuracy of the responses of participants.


```{r pdetection_group}
# Tidy for analysis
d <- d %>% 
  separate(image_name, c("nothing", "trend", "location", "type", "extra"), remove=FALSE) %>%
  select(-nothing, -extra) %>%
  mutate(location = as.numeric(location), 
    # detect measures the accuracy of the choice
         detect = ifelse(location == choice, 1, 0)) %>% 
  mutate(trend = case_when(
    trend == "nwse" ~ "NW-SE",
    trend == "cities" ~ "all cities",
    trend == "three" ~ "three cities")) %>% 
  mutate(trend = fct_relevel(trend, "NW-SE","three cities","all cities")) %>% 
  mutate(type = case_when(
    type == "hex" ~"Hex.",
    TRUE~"Choro.")) %>% 
    mutate(detect_f = factor(detect, levels = c(0,1), labels = c("No", "Yes")))

plots <- d %>% group_by(group, trend, type, location) %>%
  # pdetect measures the aggregated accuracy of the choices
  summarise(pdetect = length(detect[detect == 1])/length(detect)) 
```





```{r detection_compare, fig.cap = "The detection rates achieved by participants are contrasted when viewing the four replicates of the three trend models. Each point shows the probability of detection for the lineup display, the facets separate the trend models hidden in the lineup. The points for the same data set shown in a choroleth or haxgon tile map display are linked to show the difference in the detection rate."}
# Detectability rate for each lineup (image)
d_smry <- d %>% group_by(trend, type, replicate) %>%
  # pdetect measures the aggregated accuracy of the choices
  summarise(pdetect = length(detect[detect == 1])/length(detect)) %>%
  ungroup()

# Plot summary
ggplot(d_smry, aes(x=type, y=pdetect, colour = trend)) +
  geom_point(size = 3) +  
  geom_line(size = 1, aes(group = replicate)) +  
  facet_wrap(~trend) +  
  scale_colour_manual(values = trend_colours) +
  xlab("Type of areas visualised") +
  ylab("Detection rate") + 
  ylim(0,1) +
  guides(colour = FALSE)
```
The detection rates are usually higher when participants viewed the hexagon tile map displays. The difference was greater when viewing a three cities trend model. When the trend model affected all cities participants also detected the data display in the lineup of null plots more often. The North West to South East data plot was detected with similar rates when viewing the choropleth and  hexagon tile map displays, though for three of the replicates the participants detected the data display more often. 

```{r detection_compare_table}
d %>% 
  group_by(trend, type, replicate) %>%
  summarise(mean = round(mean(time_taken),2), 
    std.dev = round(sd(time_taken), 2)) %>% 
  knitr::kable(format = "latex")
```


```{r ttest}
# Numerical summary
diffs <- d_smry %>% spread(type, pdetect) %>%
  mutate(dif = `Hex.` - `Choro.`)

# Probability of detection using map types
test_dt <- t.test(pdetect ~ type, data = d_smry, alternative = "less")
```

A t-test shows the difference between the detection rates for the two types of displays.
The value of `r test_dt$p.value` shows that it is very unlikely the difference is due to chance. 

## Speed


```{r hist_height, fig.cap = "The time taken to evaluate each display is broken into five second windows. The height of the histogram bars show how many evaulations were submitted within each time window. The distributions for the choropleth and hexagon tile maps are very similar. Both have a large peak at 0-5 seconds, and then a secondary peak at 10-20 seconds. No response took over one minute."}
ggplot(d, aes(x = time_taken, fill = detect_f, group = detect_f)) + 
  scale_fill_brewer(palette = "Dark2") +
  geom_histogram(binwidth = 5, boundary = 0) +
  facet_grid(type ~ trend) +
  labs(x = "Time taken (Seconds)", y = "Amount of evaluations") + 
  guides(fill = FALSE)
```

```{r hist_fill, fig.cap = "The time taken to evaluate each display is broken into five second windows. The height of the histogram bars show the proportion of evaulations that were submitted within each time window."}
ggplot(d, aes(x = time_taken, fill = detect_f, group = detect_f)) + 
  scale_fill_brewer(palette = "Dark2") +
  geom_histogram(binwidth = 5, boundary = 0, position = "fill") +
  facet_grid(type ~ trend) +
  labs(x = "Time taken (Seconds)", y = "Proportion of evaluations") + 
  guides(fill = FALSE)
```



```{r}
ggplot(d, aes(x = time_taken, colour = detect_f, group = detect_f)) + 
  scale_colour_brewer(palette = "Dark2") +
  geom_freqpoly(binwidth = 3) +
  facet_grid(type ~ trend) +
  scale_fill_manual(values = detect_colours) +
  labs(x = "Time taken (Seconds)", y = "Amount of evaluations") +
  guides(colour = FALSE)
```



## Certainty

```{r, fig.cap = "The amount of times each level of certainty was chosen by participants when viewing hexagon tile map or choropleth displays. Participants were more likely to choose a high certainty when considering a Choropleth map. The default certainty of 3 was chosen most for the Hexagon tile map displays."}
d <- d %>% 
  mutate(certainty = as_factor(certainty)) 

ggplot(d,
  aes(x = certainty, fill = detect_f)) +
  scale_fill_brewer(palette = "Set2") +
  geom_bar() + facet_wrap(~trend) + guides(fill = FALSE)
```

Certainty levels are measured on a five point scale—they are subjective
assessments by the participant ‘how certain are you about your choice?’.



## Reason

```{r, fig.cap = "The most common reason for choice of plot when looking at each trend model shown in Choropleth and Hexagon Tile maps. Clusters were the most common reason when viewing a Hexagon Tile map, trend was the most common choice for choropleth displays except for the all cities display."}
###########################################################
# Qualitative analysis of reason
d <- d %>% 
  mutate(reason = fct_infreq(ifelse(reason =="0.0", "no reason", reason)))

ggplot(d) + 
  geom_bar(aes(x = reason, fill = type), position = "dodge") +  
  facet_grid(~detect_f) +  
  scale_colour_manual(values = type_colours) +
  xlab("Type of areas visualised") + 
  guides(colour = FALSE) + coord_flip()
```


## Contributors

```{r fig.cap = "The probablity of detection acheived by the contributors in each group is shown by the points. Group B has a larger range and a smaller inter-quartile range. Group A and  both had 3 people who did not find any of the data maps in the displays."}
# Check contributor performance
contribs <- d %>% group_by(group, contributor) %>% 
  # pdetect measures the aggregated accuracy of the choices
  summarise(pdetect = mean(detect, na.rm = TRUE)) 

# Plot performance
contribs %>% ggplot(aes(x = group, y = pdetect)) + 
  geom_boxplot() + 
  geom_jitter(width = 0.1, height = 0, alpha = 0.7) + 
  ylab("Detection rate") + 
  ylim(0,1)
```

## Reason

```{r}
###########################################################
# Qualitative analysis of reason
d <- d %>% 
  mutate(reason = ifelse(reason =="0.0", "no reason", reason)) 

ggplot(d %>% mutate(detect = factor(detect, levels = c(0,1), 
  labels = c("No", "Yes")))) + 
  geom_bar(aes(x = reason)) +  
  facet_grid(trend ~ detect) +  
  scale_colour_manual(values = trend_colours) +
  xlab("Type of areas visualised") + 
  guides(colour = FALSE) + coord_flip()
```


```{r choices, fig.height = 12, fig.width = 12, fig.cap = "Each facet is associated with one lineup, the height of the points show the proportion of the participants that made each choice when considering each lineup. The points coloureddark blue show the map which contained a trend model, these are the correct choices. The numbers differentiate the replicates of each trend model and type of map display. Participants were able to select 0 to indicate they did not want to choose a map. "}
d %>% 
  count(type, trend, choice, replicate, choice, detect_f) %>% 
  group_by(type, trend, replicate) %>% 
  mutate(prop = n/sum(n)) %>% 
  mutate(repl = paste(replicate, ": ", trend,  sep = "")) %>%
  mutate(bottom = 0) %>% 
  ggplot() + 
  geom_point(aes(x = choice, y = prop, color = detect_f), boundary = 0, size =2) + 
  geom_segment(aes(x = choice, xend = choice, y = bottom, yend = prop, colour = detect_f)) +
  facet_grid(replicate + type ~ trend, 
    drop = TRUE, as.table = TRUE, scales = "free_y") +
  labs(x = "Choice of plot in lineup", y = "Amount of choices") +
  scale_colour_manual(values = detect_colours) +
  scale_x_continuous(breaks = seq(from = 0, to = 12)) +
  scale_y_continuous(breaks = seq(from = 0.0, to = 1.0, by = 0.1)) +
  theme(legend.position = "bottom") + guides(colour = FALSE, fill = FALSE)
```

The choices made by participants are examined in Figure \ref{fig:choices}.
Participants were misled by the choropleth display, but not the hexagon display for all cities displays except (2). The maps with a North West to South East trend was chosen with much greater frequency in all displays. All of three cities displays, except (4), were detected in the hexagon display. All except one lineup had at least one participant select the correct map in the lineup as shown in Figure \ref{fig:choices}.


## Anomolies



## Modeling the difference

A generalized linear mixed effects model can account for each individual participants’ abilities as it includes a subject-specific random intercept. As each participant provides results from 12 lineups.

This includes the two main effects map type and trend model, which gives the fixed effects part of the model to be

$$\widehat{y_{ij}} = \mu + \tau_i + \delta_j + (\tau\delta)_{ij}, ~~~ i=1,2; ~j=1,2,3$$

where $y_{ij} = 0, 1$ whether the subject detected the data plot, $\mu$ is the overall mean, $\tau_i, i=1,2$ is the map type effect, $\delta_j$ is the trend model effect. We are allowing for an interaction between map type and trend model. Because the response is binary, a logistic model is used. 

### Detection Rates

```{r}
# Modelling 
glm1 <- glm(formula = detect_f ~ type * trend,
  family = binomial(link = "logit"), data = d)

preds <- predict(glm1, newdata = d, se=T)
pfit <- exp(preds$fit)/(1+exp(preds$fit))
se.bands.logit = tibble(upper = (preds$fit+2*preds$se.fit), lower = (preds$fit-2*preds$se.fit))
se.bands = exp(se.bands.logit)/(1+exp(se.bands.logit))

factors <- c("Design", "", "Trend", "", "Interactions", "")
terms <- c("Choro", "Hex", "Three cities", "All cities", "Hex : Three cities", "Hex : All cities")

tidy(glm1) %>% 
  mutate(p.value = round(p.value, 4)) %>%
  mutate(sig = case_when(
    p.value <= 0.001 ~ "***", 
    p.value <= 0.01 ~ "**", 
    p.value <= 0.05 ~ "*", 
    p.value <= 0.01 ~ ".",
    TRUE ~ " ")) %>% 
  mutate(term = terms,
    factors = factors) %>% 
  select(`_` = factors, 
    terms = term, 
    Estimate = estimate, 
    Error = std.error, 
    p.value, sig) %>% knitr::kable(format = "latex", 
      align = "lrrrrl")
```


```{r}
d_glm <- d %>% select(group:certainty, detect, detect_f) %>%
  bind_cols(as_tibble(se.bands)) %>% 
  mutate(pfit, 
    predicted = ifelse(pfit > 0.5, 1, 0),
    predicted = factor(predicted, levels = c(0,1), 
      labels = c("No", "Yes")))

table(d_glm$predicted, d_glm$detect_f)
```

This gives the model:


$$\widehat{y_{ij}} = 0.0217 + 0.42Hex -3.25_{three} -1.24_{all} + 2.37{Hex.three} + 1.08{Hex.all} $$



```{r}
emmean_obj1 <- emmeans(glm1, c("trend", "type"),
                        type = "response")

int_1 <- confint(emmean_obj1, by = c("trend"), adjust = "bonferroni")
int_1 %>% 
  ggplot(aes(x= type, y = prob, group = trend)) + 
  geom_point(aes(col = type)) + 
  geom_line(alpha = 0.5, lty = "dashed") + 
  geom_errorbar(aes(ymin = asymp.LCL, ymax = asymp.UCL, col = type), 
                width = 0.2) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1), 
        strip.text.y = element_text(angle = 0),
        legend.position = "none",
        axis.title.x = element_blank()) + 
  ylab("Estimated Marginal Mean")
```


```{r}
# Mixed effects models
lmer1 <- lmer(detect ~ type*trend + (1|contributor), data = d)
lmer1_d <- augment(lmer1, d)

preds_lmer <- predict(lmer1, newdata = d)
pfit_lmer <- exp(preds_lmer)/(1+exp(preds_lmer))

glance(lmer1)
tidy(lmer1)

d_lmer <- d %>% select(group:certainty, detect, detect_f) %>%
  bind_cols(as_tibble(se.bands)) %>% 
  mutate(pfit_lmer, 
    predicted = ifelse(pfit_lmer > 0.5, 1, 0),
    predicted = factor(predicted, levels = c(0,1), labels = c("No", "Yes")))

table(d_lmer$predicted, d_lmer$detect_f)


# Hexagon maps have better chance of correct detection
# Allowing for contributor effects to vary: 0.12 strong residual
```


For a base model of Choropleth map, using a NW-SE trend model.
The detection rate for Hexagon tile maps using a NW-SE trend model changes the log odds of the detection by 0.42.

```{r}
# Mixed effects models
lmer2 <- lmer(detect ~ (type|trend), data = d)
lmer2_d <- augment(lmer2, d)

preds_lmer2 <- predict(lmer2, newdata = d)
pfit_lmer2 <- exp(preds_lmer2)/(1+exp(preds_lmer2))

glance(lmer2)
tidy(lmer2)

d_lmer2 <- d %>% select(group:certainty, detect, detect_f) %>%
  bind_cols(as_tibble(se.bands)) %>% 
  mutate(pfit_lmer2, 
    predicted = ifelse(pfit_lmer2 > 0.5, 1, 0),
    predicted = factor(predicted, levels = c(0,1), labels = c("No", "Yes")))

table(d_lmer2$predicted, d_lmer2$detect_f)
```


```{r}
visreg(lmer2, "type", by = "trend", scale = "response", re.form = ~(type|trend))
```


### Certainty

```{r}
# Modelling 
lm1 <- lm(formula = as.numeric(certainty) ~ type, data = d)
lm1_d <- augment(lm1, d)
```






```{r}
# Mixed effects models
lmer1 <- lmer(detect ~ type*trend + (1|contributor), data = d)
lmer1 

# Hexagon maps have better chance of correct detection
# Allowing for contributor effects to vary: 0.12 strong residual
```


Discussion
============



Conclusion
============
how do the results found generalise to other work
- Not just for Aus (Canada new Zealand could also use this effective display)

- For USA alternative methods can also be helpful

<!-- conference papers do not normally have an appendix -->

Supplementary Materials
============

## Training

## Survey application

## Overall Performance

```{r pdetection_trend, eval = FALSE}
plots %>% ggplot(aes(x = group, y = pdetect)) + 
  geom_boxplot() + 
  geom_jitter(width = 0.1) +
  ylab("Detection rate") + 
  ylim(c(0,1))

plots %>% ggplot(aes(x = trend, y = pdetect, fill = trend)) + 
  geom_boxplot() + 
  scale_fill_manual(values = trend_colours) +
  geom_jitter(width = 0.1) +
  ylab("Detection rate") + 
  ylim(c(0,1)) 

plots %>% ggplot(aes(x = type, y = pdetect)) + 
  geom_boxplot() + 
  geom_jitter(width = 0.1) +
  ylab("Detection rate") + 
  ylim(c(0,1))
```

## Subject specific anomolies (0% detection)

## Demographics:

```{r demogs}
# Create one record for each contributor, to examine demographics
dem_contribs <- d %>%
  group_by(contributor) %>%
  slice(1) %>% ungroup()

###############################################################################
# Demographics of contributors
#dem_contribs %>% count(gender)
dem_contribs %>% ggplot() + geom_bar(aes(age))
dem_contribs %>% count(gender)
dem_contribs %>% count(education)
dem_contribs %>% count(gender, education) %>% 
  spread(gender, n) %>% 
  mutate(`Col. Total` = He + She) %>% 
  gather("Gender", "Amount", He:`Col. Total`) %>% 
  spread(education, Amount) %>% 
  mutate(`Row Total` = Bach + `High School`, Masters,
    Gender = factor(Gender, levels = c("He", "She", "Col. Total"), labels = c("He", "She", "Col. Total"))) %>% knitr::kable(format = "latex")
```


Acknowledgment {#acknowledgment}
==============

The authors would like to thank...


Ethics approval for the online survey was granted by QUT’s Ethics Committee (Ethics Application Number: 1900000991). All applicants provided informed consent in line with QUT regulations prior to participating in this research. 


Bibliography styles
===================

\newpage
References {#references .numbered}
==========





<!-- An example of a floating figure using the graphicx package. -->
<!-- Note that \label must occur AFTER (or within) \caption. -->
<!-- For figures, \caption should occur after the \includegraphics. -->
<!-- Note that IEEEtran v1.7 and later has special internal code that -->
<!-- is designed to preserve the operation of \label within \caption -->
<!-- even when the captionsoff option is in effect. However, because -->
<!-- of issues like this, it may be the safest practice to put all your -->
<!-- \label just after \caption rather than within \caption{}. -->

<!-- Reminder: the "draftcls" or "draftclsnofoot", not "draft", class -->
<!-- option should be used if it is desired that the figures are to be -->
<!-- displayed while in draft mode. -->

<!-- \begin{figure}[!t] -->
<!-- \centering -->
<!-- \includegraphics[width=2.5in]{myfigure} -->
<!-- where an .eps filename suffix were assumed under latex,  -->
<!-- and a .pdf suffix were assumed for pdflatex; or what has been declared -->
<!-- via \DeclareGraphicsExtensions. -->
<!-- \caption{Simulation results for the network.} -->
<!-- \label{fig_sim} -->
<!-- \end{figure} -->

<!-- Note that the IEEE typically puts floats only at the top, even when this -->
<!-- results in a large percentage of a column being occupied by floats. -->


<!-- An example of a double column floating figure using two subfigures. -->
<!-- (The subfig.sty package must be loaded for this to work.) -->
<!-- The subfigure \label commands are set within each subfloat command, -->
<!-- and the \label for the overall figure must come after \caption. -->
<!-- \hfil is used as a separator to get equal spacing. -->
<!-- Watch out that the combined width of all the subfigures on a  -->
<!-- line do not exceed the text width or a line break will occur. -->

<!-- \begin{figure*}[!t] -->
<!-- \centering -->
<!-- \subfloat[Case I]{\includegraphics[width=2.5in]{box}% -->
<!-- \label{fig_first_case}} -->
<!-- \hfil -->
<!-- \subfloat[Case II]{\includegraphics[width=2.5in]{box}% -->
<!-- \label{fig_second_case}} -->
<!-- \caption{Simulation results for the network.} -->
<!-- \label{fig_sim} -->
<!-- \end{figure*} -->

<!-- Note that often IEEE papers with subfigures do not employ subfigure -->
<!-- captions (using the optional argument to \subfloat[]), but instead will -->
<!-- reference/describe all of them (a), (b), etc., within the main caption. -->
<!-- Be aware that for subfig.sty to generate the (a), (b), etc., subfigure -->
<!-- labels, the optional argument to \subfloat must be present. If a -->
<!-- subcaption is not desired, just leave its contents blank, -->
<!-- e.g., \subfloat[]. -->

