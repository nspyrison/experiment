---
title: "Which is Better: a Choropleth Map or Hexagon Tile Map? A comparison using visual inference with crowdsourcing?"
affiliation:
  author-columnar: true         ## one column per author
  #institution-columnar: true  ## one column per institution (multiple autors eventually)
  # wide: true                  ## one column wide author/affiliation fields
  institution:
    - name: Queensland University of Technology
      department: Science and Engineering Faculty
      location: Brisbane, Australia
      email: stephanie.kobakian@qut.edu.au
      mark: 1
      author:
        - name: Stephanie Kobakian
    - name: Monash University
      department: Econometrics and Business Statistics Faculty
      location: Melbourne, Australia
      email: dicook@monash.edu
      mark: 1
      author:
        - name: Dianne Cook
keywords: ["statistics", "visual inference", "geospatial", "population"]
abstract: |
  The abstract goes here.
  On multiple lines eventually.
dev: png
bibliography: mybibfile.bib
output: rticles::ieee_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  error = FALSE, 
  message = FALSE)
```


Introduction
=============
<!-- no \IEEEPARstart -->
<!-- You must have at least 2 lines in the paragraph with the drop letter -->
<!-- (should never be an issue) -->
Geo-spatial statistics are often presented on the geographic map base. To present geo-spatial population statistics, information for individuals within each geographic region are often aggregated. A choropleth map is the common display to present aggreagated statistics for geographic units, and they are often used to present statistics regarding the population. 
This visualisation method involves drawing the administrative boundaries and filling them with colour to communicate the value of the statistic. 
In Australia, there are many sets of administrative boundaries that define subdivisions of the population at various granularities.


When presenting population statistics on a geographic map base, the size of the regions can allow errornous conclusions to be drawn about the state of the statistic over the entire population.
This occurs as large regions filled with a consistent colour or pettern can draw the attention of map readers, and small regions are not paid equal attention.


Background and Motivation
=============

## Australian Cancer Atlas
- Communicating spatial distributions
- Trend over geographic space
- Trend over communities and populations

The Australian Cancer Atlas explores the burden of cancer on Australian communities. There are many cancer types presented, and they can be explored on an individual or aggregate level. 
The Australian communities examined are Statistical Areas at Level 2 (SA2)[@abs2016] used by the Australian Bureau of Statistics.
Bayesian spatial smoothing has been applied to incorporate the statistics of neighbouring areas, for both privacy and stability of the estimates.
The statistics that can be mapped are the diagnoses (Standardised Incidence Rates) and excess deaths for each SA2, communicated as the difference from the Australian average of the statistics.
The values of the statistic for each are communicated using a diverging colour scheme. Dark blue represents areas with values much less than the Australian average, and represents areas much greater than the Australian average. 



## Visual Inference
- Communicating data through visualisations
- Effective displays for types of data
- Testing the effectiveness


Classical statistical inferences involves hypothesis testing, the process of rejecting a null hypothesis in favour of an alternative. This approach relies on data, the appropriate distributions and their assumptions. 




## Line up protocol

The lineup protocol presents a method for visual inference testing. 

> "In this framework, plots take on the role of test statistics, and human cognition the role of statistical tests." @SIEDAMD

The line up protocol involves placing a "guilty" data visualisation in a lineup of "innocents". Where the guilty data set contains structure, and the innocents are equivalent to a null data set. 
In a grid of visualisations, an observer is asked to pick the display that is most different, if they select the data set containing structure, they have identified the guilty hidden within the group innocents.
The guilty data is identified as different from the innocent data with probability $1/m$, where $m$ is the number of null plots plus 1 to account account for the guilty data set. When the guilty data set is chosen, the null hypothesis that it was innocent is rejected with a $1/m$ chance or type I error of being wrong.

The lineup protocol can be used in a variety of tsting scenarios. The choropleth map is best used for testing spatial structure in a data set.

<!--
The line-up protocol works like a police line-up: the suspect (test
statistic plot) is hidden in a set of decoys. If the observer, who has not
seen the suspect, can pick it out as being noticeably different, there is
evidence that it is not innocent.
-->
  
## Population focussed displays

Map creators have the ability to present spatial statistics in alternative displays that can highlight the population.
This work aims to show that a hexagon tile map display is a viable alternative to the geographic map base for presenting population statistics.
The same data were shown on a choropleth map, and on a hexagon tile map.
Comparing the results of participants who see the choropleth to those who see a hexagon tile map will show that population related distributions are spotted more frequently in a hexagon tile map display.


Study Design
============



This study aims to answer several questions around the presentation of spatial distributions:


1. Are spatial disease trends, that impact highly populated small areas, detected with higher accuracy when viewed in a hexagon tile map display?

2. Are people faster in detecting spatial disease trends, that impact highly populated small areas, when
using a hexagon tile map display?

additional considerations
 Do people find hexagon tile maps more difficult to read than choropleth maps?
 Are the reasons for choosing a plot different depending on the type of display?





## Experimental design


A survey was created to test the effectiveness of the hexagon tile map display.

The online crowdsource platform Figure-Eight was used to recruit participants. 
A line up protocol was implemented to arrange 12 maps in each display. 
Individual displays were created by a combination of plot type, and spatial trend model.

The researchers contrasted the different plot designs, as hexagon tilemap
and geography in the lineups were created using the same data, and same null positions within the lineup.

The researchers compared the length of time taken, and the accuracy of the participants choices.

## The participants

There were 95 participants in the study. 
We recruited participants using the Figure-Eight crowd source platform by advertising this survey to participants that fulfilled the following crtieria:

• level 2 or level 3 on the Figure-Eight Platform.
• at least 18 years old

Participants then selected our task from the list of tasks available to them.



## The variables being manipulated and measured


The variables that were changed between groups were the type of plot shown and the trend model.

Each participant was randomly allocated to either group A or group B when they begun the survey. This resulted in 42 participants allocated to group A, and 53 participants allocated to group B.

    
The levels of the factors measured in the experiment were:
• Plot type: *Geography, Hexagons*
• Trend: *Locations in two population centres, Locations in multiple population centres, South-East to North-West*

Factor combinations examined by each participant amount to 6 (2x3) lineup displays.
A participant did see the same data for both plot types. Four simulated sets of data were generated for each treatment.
This will generate 24 lineups (12 were geographic maps, and 12 were hexagon tile maps). Participants will evaluate 12 lineups, 6 of each plot type. Appendix A shows the experimental design visually.
For each of the six geographic displays and six hexagon displays, two of each trend model were shown to participants.


The variables measured as a result of the changes were the probability of detection each display and the time taken to submit responses.
To measure the accuracy of the detections, the plot chosen for each lineup evaluated was compared to the position of the real spatial trend plot in the lineup. A correct result occurs when the chosen plot matches the position of the real plot, this was recorded in an additional binary variable; 1 = correct; 0 = incorrect.
High efficiency occurs when a small amount of time is taken to evaluate each lineup. This will be measured as the numeric variable measuring the length of time taken to submit the answers to the evaluation of each line up.

## Participant training
Each participant saw three test displays orienting them to the task. Participants proceeded to the survey, this involved evaluating 12 displays.

## Experiment procudure and data collection
The participant submitted demographic questions and provided consent before evaluating the lineups.

Demographics were collected regarding the study participants:
• Gender (female / male / other),
• Degree education level achieved (high school / bachelors / masters / doctorate / other),
• Age range (18-24 / 25-34 / 35-44 / 45-54 / 55+ / other)
• Lived at least for one year in Australia (Yes / No )

Participants then moved to the evaulation phase.
The set of images differed for group A and group B.
After being allocated each individual was shown the 12 displays in randomised order.

Three questions were asked regarding each display:
- Plot choice
- Reason
- Difficulty

After completing the 12 evaluations, the participants were asked to submit their responses.

Data was collected through a web application containing the online survey.
Each participant used the internet to access the survey.


The data collection took place using a secure link between the survey web application and the googlesheet used to store results. The application would first connect to the googlesheet using the googlesheets [@sheets] R package, and interacted again at the completion of the survey by adding the participant's responses to the 12 displays as 12 rows of data in the googlesheet.

## The methods of data analysis used
The data analysis methods used in order to analyse and collate the results included downloading the survey submissions and opening them into the analysis software R [@RCore].
   
For each of the 12 lineup displays the researchers calculated:
• accuracy: the proportion of subjects who detected the data plot
• efficiency: average time taken to respond

### Visualisations
Side-by-side dot plots were made of accuracy (efficiency) against plot type, facetted by trend model type.

Similar plots were made of the feedback and demographic variables - reason for choice, reported difficulty, gender, age, education, having lived in Australia - against the design variables.

Plots will be made in R (R Core Team 2019), with the ggplot2 package (Wickham 2016).   
   
   
### Modeling

The results will be analysed using a generalised linear model, with a subject random effect. There will be two main effects: plot type and trend model, which gives the fixed effects part of the model to be

$$\widehat{y_{ij}} = \mu + \tau_i + \delta_j + (\tau\delta)_{ij}, ~~~ i=1,2; ~j=1,2,3$$

where $y_{ij} = 0, 1$ whether the subject detected the data plot, $\mu$ is the overall mean, $\tau_i, i=1,2$ is the plot type effect, $\delta_j$ is the trend model effect. We are allowing for an interaction between plot type and trend model. Because the response is binary, a logistic model is used.

A similar model will be constructed for the efficiency, using a log time, and normal errors. 

The feedback and demographic variables will possibly be incorporated as covariates.

Computation will be done using R [@RCore], with the `lme4` package [@lme4].


## Limitations of the data collection


This required internet connection for participants to access the survey
    


Results
============

1. Type  
1. Trend
1. Interaction of type and trend
1. Demographics of participants


```{r data}
###############################################################################

# Load Libraries
library(tidyverse)
library(lubridate)
library(broom)
library(readxl)
library(lme4)
library(ggthemes)

invthm <- theme(
  panel.background = element_rect(fill = "transparent", colour = NA), 
  plot.background = element_rect(fill = "transparent", colour = NA),
  legend.background = element_rect(fill = "transparent", colour = NA),
  legend.key = element_rect(fill = "transparent", colour = NA),
  text = element_text(colour = "white", size = 20),
  axis.text = element_text(colour = "white", size = 20)
)

# Downloaded data
d <- read_xlsx("../experiment-export.xlsx", sheet=2) %>%
  filter(!is.na(contributor)) %>%
  mutate(contributor = factor(contributor))
```



```{r}
###############################################################################
# Check data set 
# d %>% count(contributor, sort=TRUE)
# Need to clean multiple entries, 48, 24
# remove duplicated entries due to submit button
d <- d %>% group_by(group, contributor, image_name) %>%
  slice(1) %>% ungroup() %>% 
  arrange(group, contributor, plot_order)

# Now remove contributors who did not provide answers to most questions
keep <- d %>% count(contributor, sort = TRUE) %>% filter(n > 10)
d <- d %>% 
  filter(contributor %in% keep$contributor) %>%
  filter(contributor != "1234567890")

n_contributors <- d %>% count(contributor, sort=TRUE) %>% summarise(n_contributors = length(contributor))
```
The survey responses from participants were kept only if the participant submitted answers for all 12 displays. This resulted in `r n_contributors` surveyed.


```{r}
###############################################################################
# Augment
# add data replication number
#d %>% count(image_name) %>% print(n=24)
# image code <--> rep
replicate <- tibble(image_name = c("aus_cities_12_geo.png", "aus_cities_12_hex.png", 
                                   "aus_cities_3_geo.png", "aus_cities_3_hex.png",
                                   "aus_cities_4_geo.png", "aus_cities_4_hex.png",
                                   "aus_cities_9_geo.png", "aus_cities_9_hex.png",
                                   "aus_nwse_2_geo.png", "aus_nwse_2_hex.png",
                                   "aus_nwse_3_geo.png", "aus_nwse_3_hex.png",
                                   "aus_nwse_5_geo.png", "aus_nwse_5_hex.png",
                                   "aus_nwse_6_geo.png", "aus_nwse_6_hex.png",
                                   "aus_three_12_geo.png", "aus_three_12_hex.png",
                                   "aus_three_5_geo.png", "aus_three_5_hex.png",
                                   "aus_three_8_geo.png", "aus_three_8_hex.png",
                                   "aus_three_9_geo.png", "aus_three_9_hex.png"),
                    replicate = c(1, 1, 2, 2, 3, 3, 4, 4, 
                                  1, 1, 2, 2, 3, 3, 4, 4,
                                  1, 1, 2, 2, 3, 3, 4, 4))
# Add rep info to data
d <- d %>% left_join(., replicate, by = "image_name")
#d %>% count(group, image_name, sort=TRUE)`

###############################################################################
# Tidy for analysis
d <- d %>% 
  separate(image_name, c("nothing", "trend", "location", "type"), remove=FALSE) %>%
  select(-nothing) %>%
  mutate(location = as.numeric(location), 
    # detect measures the accuracy of the choice
         detect = ifelse(location == choice, 1, 0)) %>% 
  mutate(trend = case_when(
    trend == "cities" ~ "all cities",
    trend == "three" ~ "three cities",
    trend == "nwse" ~ "NW-SE")) %>% 
  mutate(trend = fct_relevel(trend, "NW-SE","three cities","all cities")) %>% 
  mutate(type = case_when(
    type == "hex" ~"Hexagons",
    TRUE~"Geography"
  )) 

plots <- d %>% group_by(group, trend, type, location) %>%
  # pdetect measures the aggregated accuracy of the choices
  summarise(pdetect = length(detect[detect == 1])/length(detect)) 

plots %>% ggplot(aes(x = group, y = pdetect)) + 
  geom_boxplot() + 
  geom_jitter(width = 0.1) +
  ylim(c(0,1))

```

```{r}
# Check contributor performance
contribs <- d %>% group_by(group, contributor) %>%
  # pdetect measures the aggregated accuracy of the choices
  summarise(pdetect = length(detect[detect == 1])/length(detect)) 

# contribs %>% count(group)
# contribs %>% arrange(pdetect)

```

```{r}
# Plot performance
contribs %>% ggplot(aes(x = group, y = pdetect, label = contributor)) + 
  geom_boxplot() + 
  geom_jitter(width = 0.1) +
  ylim(c(0,1))
```


```{r}

# Create one record for each contributor, to examine demographics
dem_contribs <- d %>%
  group_by(contributor) %>%
  slice(1) %>% ungroup()

```

```{r}


###############################################################################
# Demographics of contributors
dem_contribs %>% count(gender)
dem_contribs %>% count(age)
dem_contribs %>% count(education)
dem_contribs %>% count(australia)

```

```{r}


###############################################################################
# Detectability rate for each lineup (image)
d_smry <- d %>% group_by(trend, type, location, replicate) %>%
  # pdetect measures the aggregated accuracy of the choices
  summarise(pdetect = length(detect[detect == 1])/length(detect)) %>%
  ungroup()

# Plot summary
ggplot(d_smry, aes(x=type, y=pdetect, colour = trend)) +
  geom_point(size = 3) +  
  geom_line(size = 1, aes(group = replicate)) +  
  facet_wrap(~trend) +  
  scale_colour_brewer(palette = "Paired") +
  xlab("Type of areas visualised") +
  ylab("Detection rate") + 
  ylim(0,1) + #invthm + 
  guides(colour = FALSE)

```
11 of the 12 real distribution plots were found more often in the hexagon display.
This was better than expected. As even geographic distributions were spotted in the hexagon lineup.

```{r}
##ggsave(filename = "figures/pilot/replicate_change.png", plot = repl_plot, device = "png", dpi = 300, width = 12, height = 8, units = "in", bg = "transparent")
##ggsave(filename = "figures/pilot/replicate_change.png", plot = repl_plot, device = "png", dpi = 300, width = 12, height = 8, units = "in")

# Numerical summary
diffs <- d_smry %>% spread(type, pdetect) %>%
  mutate(dif = Hexagons - Geography)
# Need to do the t-tests for these

ggplot(diffs) + 
  geom_point(aes(Geography, Hexagons, colour = trend)) +
  geom_abline(slope = 1) +
  xlim(0,1) + ylim(0,1) + coord_equal()

# Probability of detection using map types
test_dt <- t.test(pdetect ~ type, data = d_smry, alternative = "less")

test_dt$p.value

```




```{r}

d_time <- d %>% 
  group_by(trend, type, location, replicate) %>%
  summarise(m = mean(time_taken), s = sd(time_taken))
#d_time %>% print(n=24)


ggplot(d, aes(x=time_taken, fill = trend)) +
  geom_density(alpha = 0.4) 

# Visual summary
ggplot(d_time, aes(x=type, y=m, colour = trend)) +
  geom_point(size = 3) +  
  geom_line(size = 1, aes(group = replicate)) +  
  facet_wrap(~trend) +  
  scale_colour_brewer(palette = "Paired") +
  xlab("Type of areas visualised") +
  ylab("Average time taken") + 
  guides(colour = FALSE)

#ggsave(filename = "figures/pilot/replicate_change_time.png", plot = repl_plot_t, device = "png", dpi = 300, width = 12, height = 8, units = "in", bg = "transparent")
##ggsave(filename = "figures/pilot/replicate_change.png", plot = repl_plot, device = "png", dpi = 300, width = 12, height = 8, units = "in")

```

```{r}

###########################################################
# Need to check certainty next

ggplot(d, aes(x = trend, y = certainty)) +
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75))
# certainty wasn't affected by the trend

ggplot(d %>% mutate(detect = as_factor(detect)), aes(x = detect, y = certainty)) +
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75))
# certainty was slightly higher for correct detections


```

```{r}
###########################################################
# Qualitative analysis of reason
d <- d %>% 
  mutate(reason = ifelse(reason =="0.0", "no reason", reason)) 

ggplot(d) + geom_bar(aes(x = reason)) +  
  facet_grid(trend ~ detect) +  
  scale_colour_brewer(palette = "Paired") +
  xlab("Type of areas visualised") + 
  guides(colour = FALSE) + coord_flip()


```



## Probability of detection:


## Time taken


## Modeling



Discussion
============

Conclusion
============
The conclusion goes here.

<!-- conference papers do not normally have an appendix -->

Acknowledgment {#acknowledgment}
==============

The authors would like to thank...


Ethics approval for the online survey was granted by QUT’s Ethics Committee (Ethics Application Number: 1900000991). All applicants provided informed consent in line with QUT regulations prior to participating in this research. 


Bibliography styles
===================

\newpage
References {#references .numbered}
==========





<!--This section outlines the topic and the question that the researcher aims to answer through the research.  It also outlines the reasons for researching this question, explaining how the answers could be useful or significant in some way. -->



<!-- An example of a floating figure using the graphicx package. -->
<!-- Note that \label must occur AFTER (or within) \caption. -->
<!-- For figures, \caption should occur after the \includegraphics. -->
<!-- Note that IEEEtran v1.7 and later has special internal code that -->
<!-- is designed to preserve the operation of \label within \caption -->
<!-- even when the captionsoff option is in effect. However, because -->
<!-- of issues like this, it may be the safest practice to put all your -->
<!-- \label just after \caption rather than within \caption{}. -->

<!-- Reminder: the "draftcls" or "draftclsnofoot", not "draft", class -->
<!-- option should be used if it is desired that the figures are to be -->
<!-- displayed while in draft mode. -->

<!-- \begin{figure}[!t] -->
<!-- \centering -->
<!-- \includegraphics[width=2.5in]{myfigure} -->
<!-- where an .eps filename suffix were assumed under latex,  -->
<!-- and a .pdf suffix were assumed for pdflatex; or what has been declared -->
<!-- via \DeclareGraphicsExtensions. -->
<!-- \caption{Simulation results for the network.} -->
<!-- \label{fig_sim} -->
<!-- \end{figure} -->

<!-- Note that the IEEE typically puts floats only at the top, even when this -->
<!-- results in a large percentage of a column being occupied by floats. -->


<!-- An example of a double column floating figure using two subfigures. -->
<!-- (The subfig.sty package must be loaded for this to work.) -->
<!-- The subfigure \label commands are set within each subfloat command, -->
<!-- and the \label for the overall figure must come after \caption. -->
<!-- \hfil is used as a separator to get equal spacing. -->
<!-- Watch out that the combined width of all the subfigures on a  -->
<!-- line do not exceed the text width or a line break will occur. -->

<!-- \begin{figure*}[!t] -->
<!-- \centering -->
<!-- \subfloat[Case I]{\includegraphics[width=2.5in]{box}% -->
<!-- \label{fig_first_case}} -->
<!-- \hfil -->
<!-- \subfloat[Case II]{\includegraphics[width=2.5in]{box}% -->
<!-- \label{fig_second_case}} -->
<!-- \caption{Simulation results for the network.} -->
<!-- \label{fig_sim} -->
<!-- \end{figure*} -->

<!-- Note that often IEEE papers with subfigures do not employ subfigure -->
<!-- captions (using the optional argument to \subfloat[]), but instead will -->
<!-- reference/describe all of them (a), (b), etc., within the main caption. -->
<!-- Be aware that for subfig.sty to generate the (a), (b), etc., subfigure -->
<!-- labels, the optional argument to \subfloat must be present. If a -->
<!-- subcaption is not desired, just leave its contents blank, -->
<!-- e.g., \subfloat[]. -->


<!-- An example of a floating table. Note that, for IEEE style tables, the -->
<!-- \caption command should come BEFORE the table and, given that table -->
<!-- captions serve much like titles, are usually capitalized except for words -->
<!-- such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to -->
<!-- and up, which are usually not capitalized unless they are the first or -->
<!-- last word of the caption. Table text will default to \footnotesize as -->
<!-- the IEEE normally uses this smaller font for tables. -->
<!-- The \label must come after \caption as always. -->

<!-- \begin{table}[!t] -->
<!-- % increase table row spacing, adjust to taste -->
<!-- \renewcommand{\arraystretch}{1.3} -->
<!-- if using array.sty, it might be a good idea to tweak the value of -->
<!-- \extrarowheight as needed to properly center the text within the cells -->
<!-- \caption{An Example of a Table} -->
<!-- \label{table_example} -->
<!-- \centering -->
<!-- % Some packages, such as MDW tools, offer better commands for making tables -->
<!-- % than the plain LaTeX2e tabular which is used here. -->
<!-- \begin{tabular}{|c||c|} -->
<!-- \hline -->
<!-- One & Two\\ -->
<!-- \hline -->
<!-- Three & Four\\ -->
<!-- \hline -->
<!-- \end{tabular} -->
<!-- \end{table} -->


<!-- Note that the IEEE does not put floats in the very first column -->
<!-- - or typically anywhere on the first page for that matter. Also, -->
<!-- in-text middle ("here") positioning is typically not used, but it -->
<!-- is allowed and encouraged for Computer Society conferences (but -->
<!-- not Computer Society journals). Most IEEE journals/conferences use -->
<!-- top floats exclusively.  -->
<!-- Note that, LaTeX2e, unlike IEEE journals/conferences, places -->
<!-- footnotes above bottom floats. This can be corrected via the -->
<!-- \fnbelowfloat command of the stfloats package. -->

